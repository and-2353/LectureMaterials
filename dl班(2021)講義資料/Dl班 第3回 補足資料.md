# Dl班 第3回 補足資料
###### tags: `DeepLearning班` `OSK`

## 3.1.1 ニューラルネットワークの例
図3-1 ニューラルネットワークの例(p40) を参照
中間層は隠れ層(隠れ状態ともいう)
図において、**層の部分が状態, 矢印の部分が重み**
重みを掛けたら層の部分が出力され、またそれが入力となる
この図において更新対象となっている重みは2つの行列。
だから、本文に「2層だけど3層ともよぶ」みたいな記述があるんですね～確かにどっちともとれる。

![ニューラルネットワーク](https://i.imgur.com/EP6SgJu.png)



は、たとえば

$$\begin{pmatrix}
1 & 2
\end{pmatrix}
\begin{pmatrix}
1 & 3 & 5 \\
2 & 4 & 6
\end{pmatrix}=
\begin{pmatrix}
5 & 11 & 17 \\
\end{pmatrix}$$

> 入力(1×2) ・重み(2×3) → 隠れ層(1×3)

をモデル化したもの

---

## 3.1.2 パーセプトロンの復習


本文では「$b$ は発火のしやすさをコントロールする」とさらっとかかれているけど、「$b$ は閾値を表す」みたいに明確に書かれている文献もある


それは(3.1)の式(p41)が、下の式と等しいため
$$\begin{eqnarray}
y =\left\{ \begin{array}{ll}
1 & (w_1x_1 + w_2x_2 \leqq -b) \\
0 & (w_1x_1 + w_2x_2 > -b)
\end{array} \right.
\end{eqnarray}$$

$b$ は

1. 表現力アップ( $y=w_1x_1 + w_2x_2 + b$ は $y=w_1x_1 + w_2x_2$ より表現力が高い)
2. 閾値

の2通りの解釈ができる気がする。

もちろん $b$ も更新対象。
ちなみに人のニューロンも発火しやすいものと発火しづらいものがある。
あと、何度も同じ記憶を思い出すことでニューロン同士のつながりが強化されたり、新しいつながりが生まれたりする。

---

## 3.1.3 活性化関数の登場

活性化関数の最も大きな意味は**連続での行列演算を防ぐ**こと。
どういうことかというと、行列の積は結合法則が成り立つため、2回以上繰り返しても表現力が1層と変わらなくなる。
これを防ぐために、非線形の活性化関数を間に入れる。
> 本文は、3.2.6 非線形関数でこれにふれてた。


あと、行列積は数字が大きくなるので、層を重ねるごとに数字が大きくなるのを防ぐ効果も多分ある。



## 補足 ディープラーニングの位置づけ
分類問題や回帰問題を解くためのモデルの流派の一つ。
複数の層を持ち、行列を更新するアプローチを使う。

他のアプローチには、線形判別分析、SVM、重回帰分析などがある。

なんどもいうけどディープラーニングは表現力が高いかわりに、データが多く必要だったり出力が不安定だったりといった**デメリットもある**ので、いろいろな統計分析手法を勉強して一流のデータサイエンティストをめざそう。

## 次回 やりたいこと
MNIST 実装する
普通の質的/量的データ でまず分類してみて、画像に拡張するみたいな流れがいいかも





